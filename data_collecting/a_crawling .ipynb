{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "for i in range(0, 999999999):\n",
    "    # 00 시간 00 분 00 초 000?\n",
    "    for j in range(24):\n",
    "        for k in range(60):\n",
    "            for l in range(24):\n",
    "                for m in range(999):\n",
    "                    res = str(j).zfill(2) + str(k).zfill(2) + str(l).zfill(2)+ str(m).zfill(3)\n",
    "                    \n",
    "                    url = \"https://www.bigkinds.or.kr/news/detailView.do?docId=01100101.20200617\" + res\n",
    "                    raw = requests.get(url, headers={'User-Agent' : 'Mozilla/5.0'})\n",
    "                    html = BeautifulSoup(raw.text, 'html.parser')\n",
    "                    if '미분류' in html.text:\n",
    "                        print(res)\n",
    "                    else:\n",
    "                        print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "쌩판 아이디로 하기 힘들다. 수 범위가 너무 큼.\n",
    "동적 크롤링으로 하기.\n",
    "동적 크롤링으로 해서 페이지 100페이지로 설정해서 년도별로 수집하기.\n",
    "수집 정보는 id를 클릭해 얻을 수 있는 url을 일단 전부 한 칸에 넣어서 나중에 칼럼화 하자.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(\"./chromedriver\")\n",
    "\n",
    "driver.get(\"https://www.bigkinds.or.kr/\")\n",
    "\n",
    "#search_box = driver.find_element_by_css_selector(\"input#total-search-key\")\n",
    "#search_box.send_keys(\"여성\")\n",
    "# 검색버튼 누르기 // 검색버튼: button.spm\n",
    "#search_button = driver.find_element_by_css_selector(\n",
    "#    \"button.btn main-search__search-btn news-search-btn\")\n",
    "#search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bigkinds에 주제 태그 등이 있지만 응답 시간이 너무 길어서 크롤링이 힘들다. \n",
    "네이버로 해야 겠다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "result_list=[]\n",
    "new_dict=[]\n",
    "\n",
    "for n in range(1, 1001, 10):\n",
    "    raw = requests.get(\"https://search.naver.com/search.naver?where=news&query=여성&sort=1&start=\"+str(n),\n",
    "                      headers={'User-Agent':'Mozilla/5.0'}, verify=False)\n",
    "    html = BeautifulSoup(raw.text, \"html.parser\")\n",
    "                   \n",
    "    articles = html.select(\"ul.type01 > li\")\n",
    "    \n",
    "    for ar in articles:\n",
    "        title = ar.select_one(\"a._sp_each_title\").text\n",
    "        href = ar.select_one(\"a\").get('href')\n",
    "        source = ar.select_one(\"span._sp_each_source\").text\n",
    "        \n",
    "        try:\n",
    "            h_raw = requests.get(href, headers={'User-Agent':'Mozilla/5.0'}, verify=False)\n",
    "            h_html = BeautifulSoup(h_raw.text, \"html.parser\")\n",
    "        except:\n",
    "            h_html = \"no text\"\n",
    "            \n",
    "        new_dict={\n",
    "            'title': title,\n",
    "            'source': source,\n",
    "            'text': h_html,\n",
    "        }\n",
    "        result_list.append(new_dict)\n",
    "        \n",
    "    print(n)\n",
    "    \n",
    "df = pd.DataFrame(result_list)        \n",
    "txt_name = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', f\"{'0630_estimate_size'}.xlsx\")\n",
    "excel_writer = pd.ExcelWriter(txt_name, engine='xlsxwriter')\n",
    "df.to_excel(excel_writer)\n",
    "excel_writer.save()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.01\n",
      "2019.01.02\n",
      "2019.01.03\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import calendar\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from openpyxl import load_workbook\n",
    "#from multiprocessing import Pool\n",
    "from time import sleep\n",
    "\n",
    "start_time = time.time()\n",
    "cal = calendar.Calendar()\n",
    "result_list=[]\n",
    "new_dict=[]\n",
    "\n",
    "query = \"여성\"\n",
    "\n",
    "def scrape(url):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    try:\n",
    "        res = requests.get(url, headers={'User-Agent':'Mozilla/5.0'}, allow_redirects=False)\n",
    "        return res.text\n",
    "    except:\n",
    "        return \"no text\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    book = load_workbook('xlsx#1_2019_여성.xlsx')\n",
    "    writer = pd.ExcelWriter('xlsx#1_2019_여성.xlsx', engine='openpyxl')\n",
    "    writer.book = book\n",
    "    writer.sheets = {ws.title: ws for ws in book.worksheets}            \n",
    "                \n",
    "    for month in range(1, 13):\n",
    "        monthdays = [d for d in cal.itermonthdays(2019, month) if d != 0]\n",
    "        for day in monthdays:\n",
    "            s_date = \"2019.\" + str(month).zfill(2) + \".\" + str(day).zfill(2)\n",
    "            e_date = s_date \n",
    "            s_from = s_date.replace(\".\",\"\")\n",
    "            e_to = s_from\n",
    "            print(s_date)\n",
    "\n",
    "            for n in range(1, 4001, 10):\n",
    "                url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=1&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(n)\n",
    "                raw = requests.get(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "                html = BeautifulSoup(raw.text, \"html.parser\")\n",
    "                articles = html.select(\"ul.type01 > li\")\n",
    "\n",
    "                if not articles:\n",
    "                    break\n",
    "                \n",
    "                title = list()\n",
    "                urls = list()\n",
    "                source = list()\n",
    "                date = list()\n",
    "                records = list()\n",
    "                html = list()\n",
    "                \n",
    "                for ar in articles:\n",
    "\n",
    "                    title.append(ar.select_one(\"a._sp_each_title\").text)\n",
    "                    urls.append(ar.select_one(\"a\").get('href'))\n",
    "                    source.append(ar.select_one(\"span._sp_each_source\").text)\n",
    "                    dt = str(ar.select_one(\"span.bar\").next_sibling)\n",
    "                    if dt:\n",
    "                        date.append(dt)\n",
    "                    else:\n",
    "                        date.append(s_from + \".\")\n",
    "                \n",
    "                with Pool(10) as p:\n",
    "                    records = p.map(scrape, urls)\n",
    "                \n",
    "                for re in records:\n",
    "                    html.append(BeautifulSoup(re, \"lxml\"))\n",
    "                    \n",
    "                d = pd.DataFrame({\"title\": title, \"source\": source, \"date\": date, \"html\": html}) \n",
    "                sleep(0.001)\n",
    "                    \n",
    "                #book = load_workbook('xlsx#1_2019_여성.xlsx')\n",
    "                #writer = pd.ExcelWriter('xlsx#1_2019_여성.xlsx', engine='openpyxl')\n",
    "                #writer.book = book\n",
    "                #writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "\n",
    "                for sheetname in writer.sheets:\n",
    "                    d.to_excel(writer,sheet_name=sheetname, startrow=writer.sheets[sheetname].max_row, index = False,header= False)\n",
    "                \n",
    "    writer.save()\n",
    "                \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import calendar\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from openpyxl import load_workbook\n",
    "#from multiprocessing import Pool\n",
    "from collections import ChainMap\n",
    "from time import sleep\n",
    "\n",
    "start_time = time.time()\n",
    "cal = calendar.Calendar()\n",
    "result_list=[]\n",
    "new_dict=[]\n",
    "\n",
    "query = \"여성\"\n",
    "\n",
    "#def scrape(url):\n",
    "#    import requests\n",
    "#    from bs4 import BeautifulSoup\n",
    "#    try:\n",
    "#        res = requests.get(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "        #html = BeautifulSoup(res, \"lxml\")\n",
    "#        return res.text\n",
    "#    except requests.exceptions.ConnectionError:\n",
    "#        return \"no text\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for month in range(1, 2):#13):\n",
    "        monthdays = [d for d in cal.itermonthdays(2019, month) if d != 0]\n",
    "        for day in monthdays:\n",
    "            s_date = \"2019.\" + str(month).zfill(2) + \".\" + str(day).zfill(2)\n",
    "            e_date = s_date \n",
    "            s_from = s_date.replace(\".\",\"\")\n",
    "            e_to = s_from\n",
    "            print(s_date)\n",
    "\n",
    "            for n in range(1, 10, 10):\n",
    "                url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=1&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(n)\n",
    "                raw = requests.get(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "                html = BeautifulSoup(raw.text, \"html.parser\")\n",
    "                articles = html.select(\"ul.type01 > li\")\n",
    "\n",
    "                if not articles:\n",
    "                    break\n",
    "                \n",
    "                title = list()\n",
    "                urls = list()\n",
    "                source = list()\n",
    "                date = list()\n",
    "                records = list()\n",
    "                html = list()\n",
    "                \n",
    "                for ar in articles:\n",
    "\n",
    "                    title.append(ar.select_one(\"a._sp_each_title\").text)\n",
    "                    urls.append(ar.select_one(\"a\").get('href'))\n",
    "                    source.append(ar.select_one(\"span._sp_each_source\").text)\n",
    "                    print(type(ar.select_one(\"span.bar\").next_sibling))\n",
    "                    if ar.select_one(\"span.bar\").next_sibling is not None:\n",
    "                        date.append(ar.select_one(\"span.bar\").next_sibling)\n",
    "                    else:\n",
    "                        date.append(s_from)\n",
    "                \n",
    "                #with Pool(10) as p:\n",
    "                #    records = p.map(scrape, urls)\n",
    "                \n",
    "                #for re in records:\n",
    "                #    html.append(BeautifulSoup(re, \"lxml\"))\n",
    "                for u in urls:\n",
    "                    try:\n",
    "                        r = requests.get(u, headers={'User-Agent':'Mozilla/5.0'})\n",
    "                        html.append(BeautifulSoup(r.text, \"html.parser\"))\n",
    "                    except requests.exceptions.ConnectionError:\n",
    "                        html.append(\"no text\")\n",
    "                \n",
    "                d = pd.DataFrame({\"title\": title, \"source\": source, \"date\": date, \"html\": html}) \n",
    "                sleep(0.001)\n",
    "                    \n",
    "                book = load_workbook('test2.xlsx')\n",
    "                writer = pd.ExcelWriter('test2.xlsx', engine='openpyxl')\n",
    "                writer.book = book\n",
    "                writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "\n",
    "                for sheetname in writer.sheets:\n",
    "                    d.to_excel(writer,sheet_name=sheetname, startrow=writer.sheets[sheetname].max_row, index = False,header= False)\n",
    "                \n",
    "                writer.save()\n",
    "                \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from os import getpid\n",
    "import requests \n",
    "def double(i):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    res = requests.get(i)\n",
    "    html = BeautifulSoup(res.text, \"html.parser\")\n",
    "    return res.text#(res.status_code, res.url)\n",
    "    #print(\"I'm process\")\n",
    "    #return i * 2\n",
    "res = requests.get(\"http://naver.com\")\n",
    "\n",
    "urls = [\"http://naver.com\", \"http://naver.com\"]\n",
    "if __name__ == '__main__':\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(double, urls)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing \n",
    "from itertools import repeat \n",
    "def a(x, d): \n",
    "    d[x] = True \n",
    "    \n",
    "num_cores = multiprocessing.cpu_count() \n",
    "# cpu core 개수 \n",
    "pool = multiprocessing.Pool(num_cores) \n",
    "manager = multiprocessing.Manager() \n",
    "d = manager.dict() # 프로세스간 공유할 shared dictionary \n",
    "input_list = range(0, 10) # 프로세스들에 나눠들어갈 input list \n",
    "pool.starmap(a, zip(input_list, repeat(d))\n",
    "pool.close()\n",
    "pool.join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(4)\n",
    "    results=pool.map(square,range(1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue, current_process\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "import sys\n",
    "#sys.setrecursionlimit(1000)\n",
    "\n",
    "class Spider(object):\n",
    "    \"\"\"docstring for Spider\"\"\"\n",
    "\n",
    "    # define a example function\n",
    "    @staticmethod\n",
    "    def rand_string(length, output):\n",
    "\n",
    "        print(\"{} entry point\".format(current_process().name))\n",
    "        random_post=randint(1000000,9999999)\n",
    "        response=requests.get('https://stackoverflow.com/questions/'+str(random_post))\n",
    "        print(\"{} got request response\".format(current_process().name))\n",
    "        soup=BeautifulSoup(response.content,'lxml')\n",
    "        try:\n",
    "            title = soup.find('a',{'class':'question-hyperlink'}).string\n",
    "        except:\n",
    "            title = \"not found\"\n",
    "\n",
    "        print(\"{} got title: '{}' of type: {}\".format(current_process().name, title, type(title)))\n",
    "\n",
    "        ###### This did it ######\n",
    "        title = str(title) #fix or fake news?\n",
    "\n",
    "        output.put([title,current_process().name])\n",
    "        output.close()\n",
    "        print(\"{} exit point\".format(current_process().name))\n",
    "\n",
    "\n",
    "    # Setup a list of processes that we want to run\n",
    "#    @staticmethod\n",
    "    def run(self, outq):\n",
    "        processes = []\n",
    "        for x in range(5):\n",
    "                processes.append(Process(target=self.rand_string, name=\"process_{}\".format(x), args=(x, outq,),) )\n",
    "                print(\"creating process_{}\".format(x))\n",
    "\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "            print(\"{} started\".format(p.name))\n",
    "\n",
    "        # Exit the completed processes\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "            print(\"successuflly joined {}\".format(p.name))\n",
    "\n",
    "        # Get process results from the output queue\n",
    "        print(\"joined all workers\")\n",
    "#        return None\n",
    "        out = []\n",
    "        while not outq.empty():\n",
    "            result = outq.get()\n",
    "            print(\"got {}\".format(result))\n",
    "            out.append(result)\n",
    "        return out\n",
    "\n",
    "# Run processes\n",
    "if __name__ == '__main__':\n",
    "    outq = Queue()\n",
    "    spider=Spider()\n",
    "    out = spider.run(outq)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = 'http://quotes.toscrape.com/page/'\n",
    "\n",
    "all_urls = list()\n",
    "\n",
    "def generate_urls():\n",
    "    for i in range(1,11):\n",
    "        all_urls.append(base_url + str(i))\n",
    "    \n",
    "def scrape(url):\n",
    "    import requests\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        return res.status_code, res.url\n",
    "    except:\n",
    "        return \"no\"\n",
    "\n",
    "generate_urls()\n",
    "\n",
    "p = Pool(10)\n",
    "print(all_urls)\n",
    "print(p.map(scrape, all_urls))\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " raw = requests.get(\"https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=032&listType=title&date=\"+str(date)+\"&page=\"+str(page),\n",
    "                           headers={\"User-Agent\":\"Mozilla/5.0\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
