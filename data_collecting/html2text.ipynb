{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:166: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.01.02\n",
      "2019.01.03\n",
      "2019.01.04\n",
      "2019.01.05\n",
      "2019.01.06\n",
      "2019.01.07\n",
      "2019.01.08\n",
      "2019.01.09\n",
      "2019.01.10\n",
      "2019.01.11\n",
      "2019.01.12\n",
      "2019.01.13\n",
      "2019.01.14\n",
      "2019.01.15\n",
      "2019.01.16\n",
      "2019.01.17\n",
      "2019.01.18\n",
      "2019.01.19\n",
      "2019.01.20\n",
      "2019.01.21\n",
      "2019.01.22\n",
      "2019.01.23\n",
      "2019.01.24\n",
      "2019.01.25\n",
      "2019.01.26\n",
      "2019.01.27\n",
      "2019.01.28\n",
      "2019.01.29\n",
      "2019.01.30\n",
      "2019.01.31\n",
      "2019.02.01\n",
      "2019.02.02\n",
      "2019.02.03\n",
      "2019.02.04\n",
      "2019.02.05\n",
      "2019.02.06\n",
      "2019.02.07\n",
      "2019.02.08\n",
      "2019.02.09\n",
      "2019.02.10\n",
      "2019.02.11\n",
      "2019.02.12\n",
      "2019.02.13\n",
      "2019.02.14\n",
      "2019.02.15\n",
      "2019.02.16\n",
      "2019.02.17\n",
      "2019.02.18\n",
      "2019.02.19\n",
      "2019.02.20\n",
      "2019.02.21\n",
      "2019.02.22\n",
      "2019.02.23\n",
      "2019.02.24\n",
      "2019.02.25\n",
      "2019.02.26\n",
      "2019.02.27\n",
      "2019.02.28\n",
      "2019.03.01\n",
      "2019.03.02\n",
      "2019.03.03\n",
      "2019.03.04\n",
      "2019.03.05\n",
      "2019.03.06\n",
      "2019.03.07\n",
      "2019.03.08\n",
      "2019.03.09\n",
      "2019.03.10\n",
      "2019.03.11\n",
      "2019.03.12\n",
      "2019.03.13\n",
      "2019.03.14\n",
      "2019.03.15\n",
      "2019.03.16\n",
      "2019.03.17\n",
      "2019.03.18\n",
      "2019.03.19\n",
      "2019.03.20\n",
      "2019.03.21\n",
      "2019.03.22\n",
      "2019.03.23\n",
      "2019.03.24\n",
      "2019.03.25\n",
      "2019.03.26\n",
      "2019.03.27\n",
      "2019.03.28\n",
      "2019.03.29\n",
      "2019.03.30\n",
      "2019.03.31\n",
      "2019.04.01\n",
      "2019.04.02\n",
      "2019.04.03\n",
      "2019.04.04\n",
      "2019.04.05\n",
      "2019.04.06\n",
      "2019.04.07\n",
      "2019.04.08\n",
      "2019.04.09\n",
      "2019.04.10\n",
      "2019.04.11\n",
      "2019.04.12\n",
      "2019.04.13\n",
      "2019.04.14\n",
      "2019.04.15\n",
      "2019.04.16\n",
      "2019.04.17\n",
      "2019.04.18\n",
      "2019.04.19\n",
      "2019.04.20\n",
      "2019.04.21\n",
      "2019.04.22\n",
      "2019.04.23\n",
      "2019.04.24\n",
      "2019.04.25\n",
      "2019.04.26\n",
      "2019.04.27\n",
      "2019.04.28\n",
      "2019.04.29\n",
      "2019.04.30\n",
      "2019.05.01\n",
      "2019.05.02\n",
      "2019.05.03\n",
      "2019.05.04\n",
      "2019.05.05\n",
      "2019.05.06\n",
      "2019.05.07\n",
      "2019.05.08\n",
      "2019.05.09\n",
      "2019.05.10\n",
      "2019.05.11\n",
      "2019.05.12\n",
      "2019.05.13\n",
      "2019.05.14\n",
      "2019.05.15\n",
      "2019.05.16\n",
      "2019.05.17\n",
      "2019.05.18\n",
      "2019.05.19\n",
      "2019.05.20\n",
      "2019.05.21\n",
      "2019.05.22\n",
      "2019.05.23\n",
      "2019.05.24\n",
      "2019.05.25\n",
      "2019.05.26\n",
      "2019.05.27\n",
      "2019.05.28\n",
      "2019.05.29\n",
      "2019.05.30\n",
      "2019.05.31\n",
      "2019.06.01\n",
      "2019.06.02\n",
      "2019.06.03\n",
      "2019.06.04\n",
      "2019.06.05\n",
      "2019.06.06\n",
      "2019.06.07\n",
      "2019.06.08\n",
      "2019.06.09\n",
      "2019.06.10\n",
      "2019.06.11\n",
      "2019.06.12\n",
      "2019.06.13\n",
      "2019.06.14\n",
      "2019.06.15\n",
      "2019.06.16\n",
      "2019.06.17\n",
      "2019.06.18\n",
      "2019.06.19\n",
      "2019.06.20\n",
      "2019.06.21\n",
      "2019.06.22\n",
      "2019.06.23\n",
      "2019.06.24\n",
      "2019.06.25\n",
      "2019.06.26\n",
      "2019.06.27\n",
      "2019.06.28\n",
      "2019.06.29\n",
      "2019.06.30\n",
      "2019.07.01\n",
      "2019.07.02\n",
      "2019.07.03\n",
      "2019.07.04\n",
      "2019.07.05\n",
      "2019.07.06\n",
      "2019.07.07\n",
      "2019.07.08\n",
      "2019.07.09\n",
      "2019.07.10\n",
      "2019.07.11\n",
      "2019.07.12\n",
      "2019.07.13\n",
      "2019.07.14\n",
      "2019.07.15\n",
      "2019.07.16\n",
      "2019.07.17\n",
      "2019.07.18\n",
      "2019.07.19\n",
      "2019.07.20\n",
      "2019.07.21\n",
      "2019.07.22\n",
      "2019.07.23\n",
      "2019.07.24\n",
      "2019.07.25\n",
      "2019.07.26\n",
      "2019.07.27\n",
      "2019.07.28\n",
      "2019.07.29\n",
      "2019.07.30\n",
      "2019.07.31\n",
      "2019.08.01\n",
      "2019.08.02\n",
      "2019.08.03\n",
      "2019.08.04\n",
      "2019.08.05\n",
      "2019.08.06\n",
      "2019.08.07\n",
      "2019.08.08\n",
      "2019.08.09\n",
      "2019.08.10\n",
      "2019.08.11\n",
      "2019.08.12\n",
      "2019.08.13\n",
      "2019.08.14\n",
      "2019.08.15\n",
      "2019.08.16\n",
      "2019.08.17\n",
      "2019.08.18\n",
      "2019.08.19\n",
      "2019.08.20\n",
      "2019.08.21\n",
      "2019.08.22\n",
      "2019.08.23\n",
      "2019.08.24\n",
      "2019.08.25\n",
      "2019.08.26\n",
      "2019.08.27\n",
      "2019.08.28\n",
      "2019.08.29\n",
      "2019.08.30\n",
      "2019.08.31\n",
      "2019.09.01\n",
      "2019.09.02\n",
      "2019.09.03\n",
      "2019.09.04\n",
      "2019.09.05\n",
      "2019.09.06\n",
      "2019.09.07\n",
      "2019.09.08\n",
      "2019.09.09\n",
      "2019.09.10\n",
      "2019.09.11\n",
      "2019.09.12\n",
      "2019.09.13\n",
      "2019.09.14\n",
      "2019.09.15\n",
      "2019.09.16\n",
      "2019.09.17\n",
      "2019.09.18\n",
      "2019.09.19\n",
      "2019.09.20\n",
      "2019.09.21\n",
      "2019.09.22\n",
      "2019.09.23\n",
      "2019.09.24\n",
      "2019.09.25\n",
      "2019.09.26\n",
      "2019.09.27\n",
      "2019.09.28\n",
      "2019.09.29\n",
      "2019.09.30\n",
      "2019.10.01\n",
      "2019.10.02\n",
      "2019.10.03\n",
      "2019.10.04\n",
      "2019.10.05\n",
      "2019.10.06\n",
      "2019.10.07\n",
      "2019.10.08\n",
      "2019.10.09\n",
      "2019.10.10\n",
      "2019.10.11\n",
      "2019.10.12\n",
      "2019.10.13\n",
      "2019.10.14\n",
      "2019.10.15\n",
      "2019.10.16\n",
      "2019.10.17\n",
      "2019.10.18\n",
      "2019.10.19\n",
      "2019.10.20\n",
      "2019.10.21\n",
      "2019.10.22\n",
      "2019.10.23\n",
      "2019.10.24\n",
      "2019.10.25\n",
      "2019.10.26\n",
      "2019.10.27\n",
      "2019.10.28\n",
      "2019.10.29\n",
      "2019.10.30\n",
      "2019.10.31\n",
      "2019.11.01\n",
      "2019.11.02\n",
      "2019.11.03\n",
      "2019.11.04\n",
      "2019.11.05\n",
      "2019.11.06\n",
      "2019.11.07\n",
      "2019.11.08\n",
      "2019.11.09\n",
      "2019.11.10\n",
      "2019.11.11\n",
      "2019.11.12\n",
      "2019.11.13\n",
      "2019.11.14\n",
      "2019.11.15\n",
      "2019.11.16\n",
      "2019.11.17\n",
      "2019.11.18\n",
      "2019.11.19\n",
      "2019.11.20\n",
      "2019.11.21\n",
      "2019.11.22\n",
      "2019.11.23\n",
      "2019.11.24\n",
      "2019.11.25\n",
      "2019.11.26\n",
      "2019.11.27\n",
      "2019.11.28\n",
      "2019.11.29\n",
      "2019.11.30\n",
      "2019.12.01\n",
      "2019.12.02\n",
      "2019.12.03\n",
      "2019.12.04\n",
      "2019.12.05\n",
      "2019.12.06\n",
      "2019.12.07\n",
      "2019.12.08\n",
      "2019.12.09\n",
      "2019.12.10\n",
      "2019.12.11\n",
      "2019.12.12\n",
      "2019.12.13\n",
      "2019.12.14\n",
      "2019.12.15\n",
      "2019.12.16\n",
      "2019.12.17\n",
      "2019.12.18\n",
      "2019.12.19\n",
      "2019.12.20\n",
      "2019.12.21\n",
      "2019.12.22\n",
      "2019.12.23\n",
      "2019.12.24\n",
      "2019.12.25\n",
      "2019.12.26\n",
      "2019.12.27\n",
      "2019.12.28\n",
      "2019.12.29\n",
      "2019.12.30\n",
      "2019.12.31\n"
     ]
    }
   ],
   "source": [
    "# 나중에 많이 짧은 뉴스는 생략하기. \n",
    "def html_extract(raw):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    import requests\n",
    "    \n",
    "    def replace_sub(text): #팀프하고 저 def를 이거랑 대체하면 됨. \n",
    "                \n",
    "        from nltk.tokenize import regexp_tokenize\n",
    "        from string import punctuation\n",
    "\n",
    "        text = regexp_tokenize(text, r'[^\\s\\.][^\\.\\n]+')\n",
    "        text = \".\\n\".join(text) + \".\"\n",
    "\n",
    "        text = re.sub(r'\\d+\\.(\\n)\\d+', r'', text)\n",
    "        text = re.sub(r'\\b(\\w|[.])+@(?:[.]?\\w+)+\\b', ' ', text)\n",
    "        text = re.sub(r'\\bhttps?://\\w+(?:[.]?\\w+)+\\b', ' ', text)\n",
    "        text = re.sub(r'[^A-Za-z0-9가-힣ㄱ-하-ㅣ\\.?!\\n ]', ' ', text)\n",
    "        text = re.sub(r'\\b[a-z][A-Za-z0-9]+\\b', ' ', text)\n",
    "        text = re.sub(r'\\s{2,}', ' ', text) \n",
    "        text = re.sub(r'[\\(\\[].*?[\\)\\]]', '', text)\n",
    "        text = re.sub('【.*】','',text)\n",
    "        text = re.sub(r'\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\/:;<=>\\@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~', '', text)\n",
    "        \n",
    "        try: \n",
    "            text = text[:text.rindex('다.')+2]\n",
    "            return text\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def nv_article(html):\n",
    "    \n",
    "        text = html.select_one(\"div#articleBodyContents\")\n",
    "        if not text:\n",
    "            text = html.select_one(\"div#articeBody\")\n",
    "        if text:\n",
    "            text = text.text\n",
    "            return replace_sub(text)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def etc_article(html):\n",
    "        \n",
    "        tlist = html.select('''\n",
    "            div#article-view-content-div, article#article-view-content-div, div.vcon_con, div#textBody, div.view_txt, \n",
    "            div.article.detailCont, div.cont_cont, td.view_r, div#articleText, div.txtBox, section#ndArtBody, div#post-content, \n",
    "            #main_content, div#textinput, div.rns_text, div.view_cnt, div#viewConts, div.news_contents, div#news_body_area, \n",
    "            div#newsContents, div#content, div.vcon_in.articleContent, div.contentView, div#content_body, div#view_content, \n",
    "            div#article, div.txt_article, div.article_body, div.content_print, td#ct, div#article_body, div.bodyarea, div.editor_area, \n",
    "            div#news_body_id, #container > div.con_left > div.view_left_warp > div.left_text_box, div#act-newsbody, div#print_arti,\n",
    "            div[itemprop='articleBody'], div#articleBody, .vcon_con_intxt, div#_article, div.article-news-body, div.read, .cont-body, \n",
    "            div#CmAdContent, #news_contents, section.nd-news-body, div#fontSzArea, div#font, div.entry-content, div.article, \n",
    "            div.news_content, div.col-12, #CLtag, #__newsBody__, div.viewConts, .news_bm, div.news_article, .rns_text, #divContentViewBox, \n",
    "            .wikitree_content, .content, .tipro, div.article-body, #newsContent, #news_content, .view_article, #content, .PhotoBox, \n",
    "            div.article_txt, article.contents-txt, #fade_ad_postion, div#reportDetail, #d_content, .ContentCss, .par, .text, .newscon, \n",
    "            div.cnt_view.news_body_area, .cont, div.float-right, div.newsContents, .view_cont, span.news_text, .news_con, div#GS_Content, \n",
    "            div.view_body, div.col-xs-12, .article__body, iv.smartOutput, div.cont-body, div.contents, #txt_area, \n",
    "            .contents-wrap, #container > div.inner > div.section > div.article_view > div.article_content, \n",
    "            #container > div.section_main > div.section > div > div.article > div > div, body > div.con > div.wrap2.tp2 > div.wr2_lt > div.v1d > div.v1d_con > div > div, \n",
    "            #article-view-content-div, #articleBody > div.conts, #newsView > div.rb-blog-body.rb-article, \n",
    "            #viewForm > div.media_area > div.media_body > div.text, #wrapper > div.sub-container > div.sub-layout > div.cont-article > article > div > div.cont-area, \n",
    "            div.newsTxt, body > div.vcon > div.vcon_in > div.v_lt > div > div.mi_lt > div.v1d > div.vtxt.detailCont, \n",
    "            div.news-article-memo, div#wikicon, #column div.newTemp.newType8,\n",
    "            div.article-body.clearfix, div.news_text, div.view_con, div#journal_article_wrap, div.article_view, \n",
    "            div.vtxt.detailCont, div.post-content, div.article_content, div#newscontainer, div.left_text_box,\n",
    "            div.content-list-component.text, div#view, div.cont-area, div#articleBodyContents, div.contents-wrap,\n",
    "            div.article__body, div#newsEndContents, div.detail_txt, div#contentZone td, article#articleBody,\n",
    "            div#news_body_area_contents, #news_content > div, #CmAdContent, div.cont-area, div#news_body_area, div.cnt_view.news_body_area,\n",
    "            div#textinput, div.contents_wrap, div#articleBody, #read > div.board_read, #post-14179 > div.entry-content,\n",
    "            div#viewConts, #contents_view > div.news_con, #container div.arv_001_01, div.content-area.ft-3.trix-content-area,\n",
    "            #wrapper div.cont-area > p, #post-197494 > div.post-content, #content > section.sub_view_wrap > dl,\n",
    "            #news_body_id > div.par, #articleBody > div.conts, body > div.con > div > div.wr2_lt > div.v1d > div.v1d_con,\n",
    "            div#view_content, #content, #NewsAdContent, #read_display > div,\n",
    "            #post-5031 > div > div.post-content > div.pf-content, div#article-view-content-div, #journal_article_wrap,\n",
    "            #article_content, #container  div.article_body, #news_doc > div, article#articleBody,\n",
    "            div.main-content, div#article_body, #post-255515 > div.td-post-content,\n",
    "            body > div.vcon > div.vcon_in, #contents > div, div#article_story, #container  div.arv_001_02, #newsContents,\n",
    "            #hidocBody div.ContentCss, div.dable-content-wrapper, div#content_body, #root, #articleBody, #contents div.col-md-8 > div,\n",
    "            div.wikitree_content.ar_content > p, div.txt p, div#article-view-content-div > p, div.cont-body > p,\n",
    "            font.jul, div[itemprop='articleBody'], div#newscontent, div.p-b-70, div#CLtag, article#articleContents,\n",
    "            div.rb-blog-body.rb-article, div.content_area, div.bbs-contents, div.bd_wrap.col-sm-8 tr:nth-of-type(5),\n",
    "            td#__newsBody__, div.con_area, div.viewSection, li#content_li, div.cont-body.fontsize-17 > p,\n",
    "            .bbs_cont.inr-c2 > div, .clearfix.single-box.entry-content, .entry-content.rich-content, .post_header.single, #print_area_body, \n",
    "            #ndArtBody, .article.zoominout, #bo_v_con, #dvContent div, .pf-content, .desc, #contents_Area, .content.board-content, .content-area, \n",
    "            .ABA-view-body, #bo_v_con, #contents, #entry-content, .board_view_wrap, #adiContents, td#ct\n",
    "        ''')\n",
    "        \n",
    "        \n",
    "        if tlist:\n",
    "            text = str(tlist[0])\n",
    "            text = replace_sub(text)\n",
    "            if text == 0:\n",
    "                return p_article(html)\n",
    "            else:\n",
    "                return text\n",
    "        else:\n",
    "            p_article(html)\n",
    "              \n",
    "    def p_article(html):\n",
    "        plist = html.select('p')\n",
    "        text = [t.text for t in plist]\n",
    "        text = ' '.join(text)\n",
    "        if text:\n",
    "            return replace_sub(text)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    if raw:\n",
    "        html = BeautifulSoup(raw, \"lxml\")\n",
    "        text = nv_article(html)\n",
    "\n",
    "        if not text: #여기에 네이버 외에 다른 기사 파일 정리\n",
    "            if 'moved' in raw:\n",
    "                url = html.find('a').get('href')\n",
    "                try:\n",
    "                    raw2 = requests.get(url, headers={'User-Agent':'Mozilla/5.0'}, timeout=10)\n",
    "                except:\n",
    "                    return 0\n",
    "\n",
    "                html2 = BeautifulSoup(raw2.text, \"lxml\")\n",
    "                if 'naver' in url:\n",
    "                    text = nv_article(html2)\n",
    "                else:\n",
    "                    text = etc_article(html2)\n",
    "                    \n",
    "            elif '다.' in raw:\n",
    "                text = etc_article(html)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "import os\n",
    "import re\n",
    "import openpyxl\n",
    "import calendar\n",
    "from bs4 import BeautifulSoup\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "cal = calendar.Calendar()\n",
    "\n",
    "query = \"여성\"\n",
    "query1 = \"#1_\" + query\n",
    "query2 = \"#2_\" + query\n",
    "\n",
    "year = 2019\n",
    "urllist = list()\n",
    "testlist = list()\n",
    "count = 0\n",
    "for month in range(1, 13):\n",
    "        monthdays = [d for d in cal.itermonthdays(year, month) if d != 0]\n",
    "        for day in monthdays:\n",
    "            \n",
    "            date1 = str(year) + \".\" + str(month).zfill(2) + \".\" + str(day).zfill(2)\n",
    "            date2 = date1.replace(\".\",\"\")\n",
    "            print(date1)\n",
    "            \n",
    "            xlsx_name1 = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', \"#1_crawling\", query1, '#1_'+ str(year) + '_'+query, \n",
    "                                     '{}.xlsx'.format(\"#1_\"+date2+\"_\"+query))\n",
    "            myworkbook=openpyxl.load_workbook(xlsx_name1)\n",
    "            worksheet= myworkbook.get_sheet_by_name('Sheet')\n",
    "            \n",
    "            \n",
    "            xlsx_name2 = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', '#2_html2text', query2, '#2_'+ str(year) + '_'+query, \n",
    "                                     '{}.xlsx'.format(\"#2_\"+date2+\"_\"+query))\n",
    "            wb = openpyxl.Workbook()\n",
    "            sheet = wb.active\n",
    "            sheet.append([\"title\", \"source\", \"date\", \"text\"])\n",
    "            wb.save(xlsx_name2)\n",
    "            \n",
    "            book = openpyxl.load_workbook(xlsx_name2)\n",
    "            writer = pd.ExcelWriter(xlsx_name2, engine='openpyxl')\n",
    "            writer.book = book\n",
    "            writer.sheets = {ws.title: ws for ws in book.worksheets}            \n",
    "\n",
    "\n",
    "            for i in range(2, worksheet.max_row, 10):\n",
    "                \n",
    "                title, source, date, raw, url = list(), list(), list(), list(), list()\n",
    "                \n",
    "                for j in range(10):\n",
    "                    if i+j > worksheet.max_row:\n",
    "                        break\n",
    "                    title.append(worksheet.cell(row=i+j, column=1).value)\n",
    "                    source.append(worksheet.cell(row=i+j, column=2).value)\n",
    "                    date.append(worksheet.cell(row=i+j, column=3).value)\n",
    "                    raw.append(worksheet.cell(row=i+j, column=4).value)\n",
    "                    url.append(worksheet.cell(row=i+j, column=5).value)\n",
    "                \n",
    "                with Pool(10) as p:\n",
    "                    text = p.map(html_extract, raw)\n",
    "                    \n",
    "                d = pd.DataFrame({\"title\": title, \"source\": source, \n",
    "                                      \"date\": date, \"text\": text, 'url': url})\n",
    "                \n",
    "                d = d[d.text != 0]\n",
    "                d = d.dropna()\n",
    "                \n",
    "                for sheetname in writer.sheets:\n",
    "                    d.to_excel(writer,sheet_name=sheetname, startrow=writer.sheets[sheetname].max_row, \n",
    "                               index = False,header= False)\n",
    "                \n",
    "            writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
