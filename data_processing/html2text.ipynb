{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 나중에 많이 짧은 뉴스는 생략하기.\n",
    "# 신문사 이름 지우기 가능?\n",
    "def html_extract(raw, rep):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    import requests\n",
    "    import import_ipynb\n",
    "    from sub_body import sub_body\n",
    "\n",
    "    def nv_article(html):\n",
    "    \n",
    "        text = html.select_one(\"div#articleBodyContents\")\n",
    "        if not text:\n",
    "            text = html.select_one(\"div#articeBody\")\n",
    "        if text:\n",
    "            text = text.text\n",
    "            return sub_body(text, rep)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def etc_article(html):\n",
    "        \n",
    "        tlist = html.select('''\n",
    "            div#article-view-content-div, article#article-view-content-div, div.vcon_con, div#textBody, div.view_txt, \n",
    "            div.article.detailCont, div.cont_cont, td.view_r, div#articleText, div.txtBox, section#ndArtBody, div#post-content, \n",
    "            #main_content, div#textinput, div.rns_text, div.view_cnt, div#viewConts, div.news_contents, div#news_body_area, \n",
    "            div#newsContents, div#content, div.vcon_in.articleContent, div.contentView, div#content_body, div#view_content, \n",
    "            div#article, div.txt_article, div.article_body, div.content_print, td#ct, div#article_body, div.bodyarea, div.editor_area, \n",
    "            div#news_body_id, #container > div.con_left > div.view_left_warp > div.left_text_box, div#act-newsbody, div#print_arti,\n",
    "            div[itemprop='articleBody'], div#articleBody, .vcon_con_intxt, div#_article, div.article-news-body, div.read, .cont-body, \n",
    "            div#CmAdContent, #news_contents, section.nd-news-body, div#fontSzArea, div#font, div.entry-content, div.article, \n",
    "            div.news_content, div.col-12, #CLtag, #__newsBody__, div.viewConts, .news_bm, div.news_article, .rns_text, #divContentViewBox, \n",
    "            .wikitree_content, .content, .tipro, div.article-body, #newsContent, #news_content, .view_article, #content, .PhotoBox, \n",
    "            div.article_txt, article.contents-txt, #fade_ad_postion, div#reportDetail, #d_content, .ContentCss, .par, .text, .newscon, \n",
    "            div.cnt_view.news_body_area, .cont, div.float-right, div.newsContents, .view_cont, span.news_text, .news_con, div#GS_Content, \n",
    "            div.view_body, div.col-xs-12, .article__body, iv.smartOutput, div.cont-body, div.contents, #txt_area, \n",
    "            .contents-wrap, #container > div.inner > div.section > div.article_view > div.article_content, \n",
    "            #container > div.section_main > div.section > div > div.article > div > div, body > div.con > div.wrap2.tp2 > div.wr2_lt > div.v1d > div.v1d_con > div > div, \n",
    "            #article-view-content-div, #articleBody > div.conts, #newsView > div.rb-blog-body.rb-article, \n",
    "            #viewForm > div.media_area > div.media_body > div.text, #wrapper > div.sub-container > div.sub-layout > div.cont-article > article > div > div.cont-area, \n",
    "            div.newsTxt, body > div.vcon > div.vcon_in > div.v_lt > div > div.mi_lt > div.v1d > div.vtxt.detailCont, \n",
    "            div.news-article-memo, div#wikicon, #column div.newTemp.newType8,\n",
    "            div.article-body.clearfix, div.news_text, div.view_con, div#journal_article_wrap, div.article_view, \n",
    "            div.vtxt.detailCont, div.post-content, div.article_content, div#newscontainer, div.left_text_box,\n",
    "            div.content-list-component.text, div#view, div.cont-area, div#articleBodyContents, div.contents-wrap,\n",
    "            div.article__body, div#newsEndContents, div.detail_txt, div#contentZone td, article#articleBody,\n",
    "            div#news_body_area_contents, #news_content > div, #CmAdContent, div.cont-area, div#news_body_area, div.cnt_view.news_body_area,\n",
    "            div#textinput, div.contents_wrap, div#articleBody, #read > div.board_read, #post-14179 > div.entry-content,\n",
    "            div#viewConts, #contents_view > div.news_con, #container div.arv_001_01, div.content-area.ft-3.trix-content-area,\n",
    "            #wrapper div.cont-area > p, #post-197494 > div.post-content, #content > section.sub_view_wrap > dl,\n",
    "            #news_body_id > div.par, #articleBody > div.conts, body > div.con > div > div.wr2_lt > div.v1d > div.v1d_con,\n",
    "            div#view_content, #content, #NewsAdContent, #read_display > div,\n",
    "            #post-5031 > div > div.post-content > div.pf-content, div#article-view-content-div, #journal_article_wrap,\n",
    "            #article_content, #container  div.article_body, #news_doc > div, article#articleBody,\n",
    "            div.main-content, div#article_body, #post-255515 > div.td-post-content,\n",
    "            body > div.vcon > div.vcon_in, #contents > div, div#article_story, #container  div.arv_001_02, #newsContents,\n",
    "            #hidocBody div.ContentCss, div.dable-content-wrapper, div#content_body, #root, #articleBody, #contents div.col-md-8 > div,\n",
    "            div.wikitree_content.ar_content > p, div.txt p, div#article-view-content-div > p, div.cont-body > p,\n",
    "            font.jul, div[itemprop='articleBody'], div#newscontent, div.p-b-70, div#CLtag, article#articleContents,\n",
    "            div.rb-blog-body.rb-article, div.content_area, div.bbs-contents, div.bd_wrap.col-sm-8 tr:nth-of-type(5),\n",
    "            td#__newsBody__, div.con_area, div.viewSection, li#content_li, div.cont-body.fontsize-17 > p,\n",
    "            .bbs_cont.inr-c2 > div, .clearfix.single-box.entry-content, .entry-content.rich-content, .post_header.single, #print_area_body, \n",
    "            #ndArtBody, .article.zoominout, #bo_v_con, #dvContent div, .pf-content, .desc, #contents_Area, .content.board-content, .content-area, \n",
    "            .ABA-view-body, #bo_v_con, #contents, #entry-content, .board_view_wrap, #adiContents, td#ct\n",
    "        ''')\n",
    "        \n",
    "        \n",
    "        if tlist:\n",
    "            text = [t.text for t in tlist]\n",
    "            text = ' '.join(text)\n",
    "            text = sub_body(text, rep)\n",
    "            if text == 0:\n",
    "                return p_article(html)\n",
    "            else:\n",
    "                return text\n",
    "        else:\n",
    "            p_article(html)\n",
    "              \n",
    "    def p_article(html):\n",
    "        plist = html.select('p')\n",
    "        text = [t.text for t in plist]\n",
    "        text = ' '.join(text)\n",
    "        if text:\n",
    "            return sub_body(text, rep)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    if raw:\n",
    "        html = BeautifulSoup(raw, \"lxml\")\n",
    "        text = nv_article(html)\n",
    "\n",
    "        if not text: #여기에 네이버 외에 다른 기사 파일 정리\n",
    "            if 'moved' in raw:\n",
    "                url = html.find('a').get('href')\n",
    "                try:\n",
    "                    raw2 = requests.get(url, headers={'User-Agent':'Mozilla/5.0'}, timeout=10)\n",
    "                except:\n",
    "                    return 0\n",
    "\n",
    "                html2 = BeautifulSoup(raw2.text, \"lxml\")\n",
    "                if 'naver' in url:\n",
    "                    text = nv_article(html2)\n",
    "                else:\n",
    "                    text = etc_article(html2)\n",
    "                    \n",
    "            elif '다.' in raw:\n",
    "                text = etc_article(html)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import openpyxl\n",
    "import calendar\n",
    "from bs4 import BeautifulSoup\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "import pandas as pd\n",
    "import time\n",
    "import import_ipynb\n",
    "from rep_body import rep_body\n",
    "\n",
    "rep = rep_body()\n",
    "\n",
    "cal = calendar.Calendar()\n",
    "\n",
    "query = \"여성\"\n",
    "query1 = \"#1_\" + query\n",
    "query2 = \"#2_\" + query\n",
    "\n",
    "urllist, testlist = list(), list()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "print(\"hello\")\n",
    "\n",
    "dir1 = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', \"#2_html2text\", query2)\n",
    "if not os.path.exists(dir1):\n",
    "    os.makedirs(dir1)\n",
    "\n",
    "for year in range(2000, 2020):\n",
    "    dir2 = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', \"#2_html2text\", query2, \n",
    "                   '#2_'+ str(year) + '_'+query)\n",
    "    if not os.path.exists(dir2):\n",
    "        os.makedirs(dir2)\n",
    "        \n",
    "    for month in range(1, 13):\n",
    "            monthdays = [d for d in cal.itermonthdays(year, month) if d != 0]\n",
    "            for day in monthdays:\n",
    "\n",
    "                date1 = str(year) + \".\" + str(month).zfill(2) + \".\" + str(day).zfill(2)\n",
    "                date2 = date1.replace(\".\",\"\")\n",
    "                print(date1)\n",
    "\n",
    "                xlsx_name1 = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', \n",
    "                                          \"#1_crawling\", query1, '#1_'+ str(year) + '_'+query, \n",
    "                                         '{}.xlsx'.format(\"#1_\"+date2+\"_\"+query))\n",
    "                myworkbook=openpyxl.load_workbook(xlsx_name1)\n",
    "                worksheet= myworkbook.get_sheet_by_name('Sheet')\n",
    "\n",
    "\n",
    "                xlsx_name2 = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', \n",
    "                                          '#2_html2text', query2, '#2_'+ str(year) + '_'+query, \n",
    "                                         '{}.xlsx'.format(\"#2_\"+date2+\"_\"+query))\n",
    "                wb = openpyxl.Workbook()\n",
    "                sheet = wb.active\n",
    "                sheet.append([\"title\", \"source\", \"date\", \"text\"])\n",
    "                wb.save(xlsx_name2)\n",
    "\n",
    "                book = openpyxl.load_workbook(xlsx_name2)\n",
    "                writer = pd.ExcelWriter(xlsx_name2, engine='openpyxl')\n",
    "                writer.book = book\n",
    "                writer.sheets = {ws.title: ws for ws in book.worksheets}            \n",
    "\n",
    "\n",
    "                for i in range(2, worksheet.max_row, 10):\n",
    "\n",
    "                    title, source, date, raw = list(), list(), list(), list()\n",
    "                    rlist, plist = list(), list()\n",
    "                    \n",
    "                    for j in range(10):\n",
    "                        if i+j > worksheet.max_row:\n",
    "                            break\n",
    "                        title.append(worksheet.cell(row=i+j, column=1).value)\n",
    "                        source.append(worksheet.cell(row=i+j, column=2).value)\n",
    "                        date.append(worksheet.cell(row=i+j, column=3).value)\n",
    "                        raw.append(worksheet.cell(row=i+j, column=4).value)\n",
    "                        rlist.append(rep)\n",
    "                        \n",
    "                    with Pool(10) as p:\n",
    "                        text = p.map(html_extract, raw, rlist)\n",
    "\n",
    "                    d = pd.DataFrame({\"title\": title, \"source\": source, \n",
    "                                          \"date\": date, \"text\": text})\n",
    "\n",
    "                    d = d[d.text != 0]\n",
    "                    d = d.dropna()\n",
    "\n",
    "                    for sheetname in writer.sheets:\n",
    "                        d.to_excel(writer,sheet_name=sheetname, startrow=writer.sheets[sheetname].max_row, \n",
    "                                   index = False,header= False)\n",
    "\n",
    "                writer.save()\n",
    "\n",
    "                \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
