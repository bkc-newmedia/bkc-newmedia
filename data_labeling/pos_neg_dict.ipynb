{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "knu로 추출한 상위 기사에서, 새로운 긍정/부정 사전을 생성한다. \n",
    "사전은 동사와 형용사로 이루어져 있다. \n",
    "tf-idf로 word와 polarity를 형성한다. \n",
    "긍정기사에서 tf-idf값이 클 수록 긍정인 단어, \n",
    "부정기사에서 tf-idf값이 낮을 수록 부정인 단어라 가정한다. \n",
    "tf-idf값 필터링의 기준을 0.2에서부터 다양하게 바꾸어 본다.\n",
    "평균보다 빈도가 높은 단어만 포함시켜 사전의 질을 향상시킨다. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from eunjeon import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "#stopword = ['NNBC', 'SF', 'SE', 'SSO', 'SSC', 'SC', 'SY', 'SL', 'SH', 'SN', 'JKS', 'JKC', 'JKG',\n",
    "#           'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC', 'NNP', 'NNG', 'NNB','NR', 'NP']\n",
    "\n",
    "def generic_regroup(values, keys):\n",
    "    groups = dict()\n",
    "    valkeys = [k for k in values[0] if k not in keys]\n",
    "    for d in values:\n",
    "        key = tuple(d[k] for k in keys)\n",
    "        if key in groups:\n",
    "            group = groups[key]\n",
    "            for k in valkeys:\n",
    "                group[k] += d[k]\n",
    "        else:\n",
    "            groups[key] = d.copy()\n",
    "    return list(groups.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_name = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', '#3_knu_score', f\"{'knu_score_pos'}.xlsx\")\n",
    "df_pos = pd.read_excel(df_pos_name)\n",
    "\n",
    "p_corpus, p_result = list(), list()\n",
    "\n",
    "for i in range(len(df_pos)):\n",
    "    text = (str(df_pos.loc[i, 'title'])+\".\\n\"+str(df_pos.loc[i, 'text'])).split('.\\n')\n",
    "    p_corpus.append( \". \".join([\" \".join([f[0] for f in mecab.pos(e) if \"VV\" in f[1] or \"VA\" in f[1]]) for e in text]))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "sp_matrix = vectorizer.fit_transform(p_corpus)\n",
    "\n",
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx\n",
    "\n",
    "\n",
    "for i, sent in enumerate(p_corpus):\n",
    "    seen = set()\n",
    "    temp =  [ {'word': token, 'polarity': sp_matrix[i, word2id[token]], 'count': 1} \n",
    "               for token in sent.split() if sp_matrix[i, word2id[token]] > 0.02] # 0.02\n",
    "    p_result.extend([x for x in temp if not (x['word'] in seen or seen.add(x['word']))])\n",
    "\n",
    "p_res = generic_regroup(p_result, [\"word\"])\n",
    "p_avg = sum([x['count'] for x in p_res]) / len(p_res)\n",
    "p_res = [x for x in p_res if x['count'] > p_avg/2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg_name = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', '#3_knu_score', f\"{'knu_score_neg'}.xlsx\")\n",
    "df_neg = pd.read_excel(df_neg_name)\n",
    "\n",
    "n_corpus, n_result = list(), list()\n",
    "\n",
    "for i in range(len(df_neg)):\n",
    "    text = (str(df_neg.loc[i, 'title'])+\".\\n\"+str(df_neg.loc[i, 'text'])).split('.\\n')\n",
    "    n_corpus.append( \". \".join([\" \".join([f[0] for f in mecab.pos(e) if \"VV\" in f[1] or \"VA\" in f[1]]) for e in text]))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "sp_matrix = vectorizer.fit_transform(n_corpus)\n",
    "\n",
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx\n",
    "\n",
    "\n",
    "for i, sent in enumerate(n_corpus):\n",
    "    seen = set()\n",
    "    temp =  [ {'word': token, 'polarity': -sp_matrix[i, word2id[token]], 'count': 1} \n",
    "               for token in sent.split() if sp_matrix[i, word2id[token]] > 0.02] # 0.02\n",
    "    n_result.extend([x for x in temp if not (x['word'] in seen or seen.add(x['word']))])\n",
    "\n",
    "n_res = generic_regroup(n_result, [\"word\"])\n",
    "n_avg = sum([x['count'] for x in n_res]) / len(n_res)\n",
    "n_res = [x for x in n_res if x['count'] > n_avg/2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_value = [p['word'] for p in p_res]\n",
    "neg_value = [n['word'] for n in n_res]\n",
    "z = set(pos_value).intersection(set(neg_value)) \n",
    "\n",
    "pos_json = [x for x in p_res if x['word'] not in z]\n",
    "\n",
    "neg_json = [x for x in n_res if x['word'] not in z]\n",
    "\n",
    "\n",
    "total = pos_json + neg_json\n",
    "\n",
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', '#3_new_dict_score', f\"{'pos_neg_dict'}.json\"), \"w\", encoding='utf-8') as outfile:\n",
    "     json.dump(total, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dff = pd.read_json(os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', '#3_new_dict_score', f\"{'pos_neg_dict'}.json\"))\n",
    "dff['divided'] = (dff['polarity'] / dff['count']) * 2\n",
    "dff.to_excel(\"neg_pos_dict_1.xlsx\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
