{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "knu로 추출한 상위 기사에서, 새로운 긍정/부정 단어장을 생성한다. \n",
    "tf-idf로 word와 polarity를 형성한다. \n",
    "tf-idf의 기준을 0.2에서부터 다양하게 바꾸어 본다.\n",
    "setiment_score에서 사용 시에는 polarity를 count로 나누도록 한다. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from eunjeon import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "stopword = ['NNBC', 'SF', 'SE', 'SSO', 'SSC', 'SC', 'SY', 'SL', 'SH', 'SN', 'JKS', 'JKC', 'JKG',\n",
    "           'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC', 'NNP']\n",
    "\n",
    "\n",
    "def generic_regroup(values, keys):\n",
    "    groups = dict()\n",
    "    valkeys = [k for k in values[0] if k not in keys]\n",
    "    for d in values:\n",
    "        key = tuple(d[k] for k in keys)\n",
    "        if key in groups:\n",
    "            group = groups[key]\n",
    "            for k in valkeys:\n",
    "                group[k] += d[k]\n",
    "        else:\n",
    "            groups[key] = d.copy()\n",
    "    return list(groups.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_name = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', '#3_knu_score', f\"{'knu_score_pos'}.xlsx\")\n",
    "df_pos = pd.read_excel(df_pos_name)\n",
    "\n",
    "p_corpus, p_result = list(), list()\n",
    "\n",
    "for i in range(len(df_pos)):\n",
    "    text = (str(df_pos.loc[i, 'title'])+\".\\n\"+str(df_pos.loc[i, 'text'])).split('.\\n')\n",
    "    p_corpus.append( \". \".join([\" \".join([f[0] for f in mecab.pos(e) if not f[1] in stopword]) for e in text]))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "sp_matrix = vectorizer.fit_transform(p_corpus)\n",
    "\n",
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx\n",
    "\n",
    "\n",
    "for i, sent in enumerate(p_corpus):\n",
    "    seen = set()\n",
    "    temp =  [ {'word': token, 'polarity': sp_matrix[i, word2id[token]], 'count': 1} \n",
    "               for token in sent.split() if sp_matrix[i, word2id[token]] > 0.05 and sp_matrix[i, word2id[token]] < 0.5] # 0.02\n",
    "    p_result.extend([x for x in temp if not (x['word'] in seen or seen.add(x['word']))])\n",
    "\n",
    "p_res = generic_regroup(p_result, [\"word\"])\n",
    "p_res = [x for x in n_res if x['count'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg_name = os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', '#3_knu_score', f\"{'knu_score_neg'}.xlsx\")\n",
    "df_neg = pd.read_excel(df_neg_name)\n",
    "\n",
    "n_corpus, n_result = list(), list()\n",
    "\n",
    "for i in range(len(df_neg)):\n",
    "    text = (str(df_neg.loc[i, 'title'])+\".\\n\"+str(df_neg.loc[i, 'text'])).split('.\\n')\n",
    "    n_corpus.append( \". \".join([\" \".join([f[0] for f in mecab.pos(e) if not f[1] in stopword]) for e in text]))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "sp_matrix = vectorizer.fit_transform(n_corpus)\n",
    "\n",
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx\n",
    "\n",
    "\n",
    "for i, sent in enumerate(n_corpus):\n",
    "    seen = set()\n",
    "    temp =  [ {'word': token, 'polarity': -sp_matrix[i, word2id[token]], 'count': 1} \n",
    "               for token in sent.split() if sp_matrix[i, word2id[token]] > 0.05 and sp_matrix[i, word2id[token]] < 0.5 ] # 0.02\n",
    "    n_result.extend([x for x in temp if not (x['word'] in seen or seen.add(x['word']))])\n",
    "\n",
    "n_res = generic_regroup(n_result, [\"word\"])\n",
    "n_res = [x for x in n_res if x['count'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#조사 불용어에 포함\n",
    "#f오류 해결\n",
    "#e딕셔너리 h해결\n",
    "#긍정/부정에 동시에 나오는 단어 제거\n",
    "#대명사 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_value = [p['word'] for p in p_res]\n",
    "neg_value = [n['word'] for n in n_res]\n",
    "z = set(pos_value).intersection(set(neg_value)) \n",
    "\n",
    "pos_json = [x for x in p_res if x['word'] not in z or \n",
    "            (x['word'] in z and (np.abs(x['polarity'])/np.sqrt(x['count'])) > \n",
    "                                 (np.abs(n_res[neg_value.index(x['word'])]['polarity']))\n",
    "                                 /np.sqrt(n_res[neg_value.index(x['word'])]['count']))]\n",
    "\n",
    "neg_json = [x for x in n_res if x['word'] not in z or \n",
    "            (x['word'] in z and (np.abs(x['polarity'])/np.sqrt(x['count'])) > \n",
    "                                 (np.abs(p_res[pos_value.index(x['word'])]['polarity']))\n",
    "                                 /np.sqrt(p_res[pos_value.index(x['word'])]['count']))]\n",
    "\n",
    "\n",
    "total = pos_json + neg_json\n",
    "\n",
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', '#3_new_dict_score', f\"{'pos_neg_dict'}.json\"), \"w\", encoding='utf-8') as outfile:\n",
    "     json.dump(total, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_json(os.path.join(os.path.dirname(os.getcwd()), 'xlsx_data', '#3_new_dict_score', f\"{'pos_neg_dict'}.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['divided'] = dff['polarity'] / np.sqrt(dff['count'])\n",
    "dff.to_excel(\"neg_pos_dict.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
